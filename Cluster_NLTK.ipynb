{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCTg9426TdrQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "WPb04Gq3TgqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n"
      ],
      "metadata": {
        "id": "hXA55y4sUdxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download pt_core_news_lg\n",
        "nlp = spacy.load(\"pt_core_news_lg\")"
      ],
      "metadata": {
        "id": "fAdzAlrYyykm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separador de sentenças\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Base de dados léxica\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')\n",
        "\n",
        "#Pos-tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "id": "exe08XNRWeWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura dos dados"
      ],
      "metadata": {
        "id": "H1GNG_aNTrLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getText(fileName):\n",
        "    pdf = open('/content/drive/MyDrive/dados/artigos/'+fileName, 'rb')\n",
        "    reader = PdfReader(pdf)\n",
        "    text = []\n",
        "    for i in range(0,len(reader.pages)):\n",
        "        text.append(reader.pages[i].extract_text().replace('\\t', ' '))\n",
        "    pdf.close()\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "files = [getText('file'+str(i)+'.pdf') for i in range(0,37)]"
      ],
      "metadata": {
        "id": "F8BCz-2wTg13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré processamento\n",
        "\n",
        "Some of the common text preprocessing / cleaning steps are:\n",
        "\n",
        "- Normalization\n",
        "- Lower casing\n",
        "- Removal of Punctuations\n",
        "- Removal of Stopwords\n",
        "- Removal of Frequent words\n",
        "- Removal of Rare words\n",
        "- Stemming\n",
        "- Lemmatization\n",
        "- Removal of emojis\n",
        "- Removal of emoticons\n",
        "- Conversion of emoticons to words\n",
        "- Conversion of emojis to words\n",
        "- Removal of URLs\n",
        "- Removal of HTML tags\n",
        "- Chat words conversion\n",
        "- Spelling correction\n",
        "\n",
        "So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role."
      ],
      "metadata": {
        "id": "MCO62KPpT_bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\u00C7\", \"\\u0043\"+\"\\u0327\")"
      ],
      "metadata": {
        "id": "EJ1_DwuC0xID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"CACHA\\u00C7A\"\n",
        "word2 = \"CACHA\\u0043\\u0327A\"\n",
        "\n",
        "print(word1, word2)"
      ],
      "metadata": {
        "id": "FC-2kUxe1jR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 == word2"
      ],
      "metadata": {
        "id": "VJaNTcsG1jgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decompoe em componentes menores:\n",
        "# \\u00C7 => \\u0043 + \\u0327\n",
        "unicodedata.normalize('NFD', word1) == word2"
      ],
      "metadata": {
        "id": "n8ODdcr63JUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decompoe em componentes menores seguido de composição\n",
        "#\\u00C7 => \\u0043 + \\u0327 => \\u00C7\n",
        "#No exemplo: \\u0043 + \\u0327 => \\u00C7\n",
        "unicodedata.normalize('NFC', word2) == word1"
      ],
      "metadata": {
        "id": "idoOhVH33cI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decompoe na sua versão normal\n",
        "unicodedata.normalize('NFKD', word1) == unicodedata.normalize('NFKD', word2)"
      ],
      "metadata": {
        "id": "y0U2ljfm3vU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"O plenário do     Senado aprovou hoje no dia 08 a proposta de reforma tributária, uma das principais pautas da agenda econômica do governo Lula (PT), com o apoio das bancadas do centrão. Como o texto sofreu alterações, o projeto voltará para mais uma análise da Câmara dos Deputados.\"\n"
      ],
      "metadata": {
        "id": "qDsK4BXc0ukp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar caracteres acentuados para suas formas não acentuadas\n",
        "texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
        "print(texto)"
      ],
      "metadata": {
        "id": "gcfb31zr0zEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove caracteres não alfanuméricos\n",
        "texto = re.sub(r'[^\\w\\sáÁâÂãÃàÀéÉêÊíÍóÓôÔõÕúÚçÇ]', ' ', texto)\n",
        "print(texto)\n",
        "\n",
        "# Remove números\n",
        "texto = ''.join([i for i in texto if not i.isdigit()])\n",
        "print(texto)\n",
        "\n",
        "# Remove espaços em branco adicionais\n",
        "texto = re.sub('[\\s]+', ' ', texto)\n",
        "texto = re.sub('[\\n]+', ' ', texto)\n",
        "print(texto)"
      ],
      "metadata": {
        "id": "Q3idsYvA1Bwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "s = sent_tokenize(texto)\n",
        "t = [word_tokenize(sent) for sent in s]\n",
        "print(texto)\n",
        "print(s)\n",
        "print(t)\n"
      ],
      "metadata": {
        "id": "T_B-v2h9qwVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_tokens= [token for sentence_tokens in t for token in sentence_tokens if token.lower() not in stop_words]\n",
        "\n",
        "print(\"Com stopwords:\", t)\n",
        "print(\"Sem stopwords:\", s_tokens)"
      ],
      "metadata": {
        "id": "3FG9B_2e7-2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer_tokens1 = [lemmatizer.lemmatize(token) for token in s_tokens]\n",
        "\n",
        "doc = nlp(' '.join(s_tokens))\n",
        "lemmatizer_tokens2 = [token.lemma_ for token in doc]\n",
        "\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "stemmer_tokens1 = [stemmer.stem(token) for token in s_tokens]\n",
        "\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "stemmer_tokens2 = [stemmer.stem(token) for token in s_tokens]\n",
        "\n",
        "print(\"Lemmatizer:\", lemmatizer_tokens1)\n",
        "print(\"Lemmatizer:\", lemmatizer_tokens2)\n",
        "\n",
        "print(\"Stemmer:\", stemmer_tokens1)\n",
        "print(\"Stemmer:\", stemmer_tokens2)\n"
      ],
      "metadata": {
        "id": "1VcvX4PCvt5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcessing(text):\n",
        "\n",
        "    # Normalizar caracteres acentuados para suas formas não acentuadas\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "\n",
        "    # Remove caracteres não alfanuméricos\n",
        "    text = re.sub(r'[^\\w\\sáÁâÂãÃàÀéÉêÊíÍóÓôÔõÕúÚçÇ]', ' ', text)\n",
        "\n",
        "    # Remove números\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "\n",
        "    # Remove espaços em branco adicionais\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "    text = re.sub('[\\n]+', ' ', text)\n",
        "\n",
        "    # Tokenização\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # Converter para minúsculas\n",
        "    tokens = [[token.lower() for token in sent if token.isalpha()] for sent in tokens]\n",
        "\n",
        "    # Remover pontuação\n",
        "    tokens = [[token for token in sent if token not in string.punctuation] for sent in tokens]\n",
        "\n",
        "    # Remoção de stopwords\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    stopwords_tokens = [[token for token in sent if token not in stop_words] for sent in tokens]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = SnowballStemmer(\"portuguese\")\n",
        "    transformed_tokens = [[stemmer.stem(token) for token in sent] for sent in stopwords_tokens]\n",
        "    return transformed_tokens\n",
        "\n",
        "    # Lemmatization\n",
        "    #transformed_tokens = []\n",
        "    #lemmatizer = WordNetLemmatizer()\n",
        "    #transformed_tokens = [[lemmatizer.lemmatize(token) for token in sent] for sent in stopwords_tokens]\n",
        "    #return transformed_tokens\n",
        "\n",
        "    #texto = [sent for sent in stopwords_tokens[0]]\n",
        "    #texto = ' '.join(texto)\n",
        "    #doc = nlp(texto)\n",
        "    #transformed_tokens = [token.lemma_ for token in doc]\n",
        "    #return [transformed_tokens]\n"
      ],
      "metadata": {
        "id": "_eVjq5UtThBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Montagem do corpus"
      ],
      "metadata": {
        "id": "lbBslYSrUQaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createCorpus(files):\n",
        "    # Cria diretório para salvar o corpus processado\n",
        "    path = 'corpus/'\n",
        "    if not os.path.isdir(path): os.mkdir(path)\n",
        "\n",
        "    for idx, file in enumerate(files):\n",
        "        preprocessed_tokens = preProcessing(file)\n",
        "\n",
        "        # Grava os dados processados em arquivo texto\n",
        "        with open(path + str(idx) + '.txt', 'w') as fout:\n",
        "            fout.write('\\n'.join([' '.join(sent) for sent in preprocessed_tokens]))\n",
        "\n",
        "    return PlaintextCorpusReader(path, '.*')\n",
        "\n",
        "\n",
        "def showCorpus(corpus):\n",
        "  for file_id in corpus.fileids():\n",
        "    print(\"File:\", file_id)\n",
        "    tokens = corpus.words(file_id)\n",
        "    print(tokens)\n",
        "    print(nltk.pos_tag(tokens))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "corpus = createCorpus(files)\n",
        "showCorpus(corpus)\n",
        "#open('corpus/0.txt', 'r').read()"
      ],
      "metadata": {
        "id": "PNPjuqHoUUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agrupamento de textos por afinidade"
      ],
      "metadata": {
        "id": "Qs-d9aE2XuhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura do corpus\n",
        "path = 'corpus/'\n",
        "corpus = PlaintextCorpusReader(path, '.*')\n",
        "\n",
        "# Obter os textos pré-processadas\n",
        "text = [corpus.raw(file) for file in corpus.fileids()]\n",
        "\n",
        "print(\"- \", text[0])\n",
        "print(\"- \", text[1])\n",
        "print(\"- \", text[36])"
      ],
      "metadata": {
        "id": "rYe1T4GYg7ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma matriz TF-IDF\n",
        "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "GymmGlwahPd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupamento K-Means\n",
        "number_clusters = 6\n",
        "kmeans = KMeans(n_clusters=number_clusters, init='k-means++', algorithm='lloyd')\n",
        "kmeans.fit(X)\n",
        "\n",
        "for cluster_id in range(number_clusters):\n",
        "    print(f\"Cluster {cluster_id + 1}:\")\n",
        "    cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
        "    for idx in cluster_indices:\n",
        "        print(corpus.fileids()[idx])\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "ilD59Oc1Xx8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuvem de palavras"
      ],
      "metadata": {
        "id": "w9CnRMbvdpTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "def showWordCloud(nameFile):\n",
        "    file = getText(nameFile)\n",
        "\n",
        "    tokens = preProcessing(file)\n",
        "    frequency_dist = nltk.FreqDist(tokens[0])\n",
        "\n",
        "    wcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(wcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qnuv6nttJRpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showWordCloud('file0.pdf')"
      ],
      "metadata": {
        "id": "N4PcPVqhdzPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showWordCloud('file10.pdf')"
      ],
      "metadata": {
        "id": "xHAFe-6gf4bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showWordCloud('file35.pdf')"
      ],
      "metadata": {
        "id": "83NLiehBf4og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words"
      ],
      "metadata": {
        "id": "Rh-oO6i4jJ41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "doc1 = getText('file0.pdf')\n",
        "doc2 = getText('file10.pdf')\n",
        "doc3 = getText('file35.pdf')\n",
        "\n",
        "\n",
        "#path = 'corpus/'\n",
        "#corpus = PlaintextCorpusReader(path, '.*')\n",
        "#text = [corpus.raw(file) for file in corpus.fileids()]\n",
        "#doc1 = text[0]\n",
        "#doc2 = text[10]\n",
        "#doc3 = text[35]\n",
        "\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "\n",
        "X = bow_vectorizer.fit_transform([doc1,doc2,doc3])\n",
        "\n",
        "bow_df = pd.DataFrame(X.toarray(),columns=bow_vectorizer.get_feature_names_out())\n",
        "bow_df.head()"
      ],
      "metadata": {
        "id": "nmHlJ4ADjKCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando as palavras mais frequentes"
      ],
      "metadata": {
        "id": "h9YH3xVwl3je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file_id in corpus.fileids():\n",
        "    print(\"File:\", file_id)\n",
        "\n",
        "    tokens = corpus.words(file_id)\n",
        "    frequencia = nltk.FreqDist(tokens)\n",
        "    most_common = frequencia.most_common(5)\n",
        "\n",
        "    print(most_common)\n",
        "    print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "NV7M9IJ_0bhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}